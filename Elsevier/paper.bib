@article{Zhu2019,
abstract = {Rationale and objectives: To determine whether deep learning models can distinguish between breast cancer molecular subtypes based on dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI). Materials and methods: In this institutional review board–approved single-center study, we analyzed DCE-MR images of 270 patients at our institution. Lesions of interest were identified by radiologists. The task was to automatically determine whether the tumor is of the Luminal A subtype or of another subtype based on the MR image patches representing the tumor. Three different deep learning approaches were used to classify the tumor according to their molecular subtypes: learning from scratch where only tumor patches were used for training, transfer learning where networks pre-trained on natural images were fine-tuned using tumor patches, and off-the-shelf deep features where the features extracted by neural networks trained on natural images were used for classification with a support vector machine. Network architectures utilized in our experiments were GoogleNet, VGG, and CIFAR. We used 10-fold crossvalidation method for validation and area under the receiver operating characteristic (AUC)as the measure of performance. Results: The best AUC performance for distinguishing molecular subtypes was 0.65 (95{\%} CI:[0.57,0.71])and was achieved by the off-the-shelf deep features approach. The highest AUC performance for training from scratch was 0.58 (95{\%} CI:[0.51,0.64])and the best AUC performance for transfer learning was 0.60 (95{\%} CI:[0.52,0.65])respectively. For the off-the-shelf approach, the features extracted from the fully connected layer performed the best. Conclusion: Deep learning may play a role in discovering radiogenomic associations in breast cancer.},
archivePrefix = {arXiv},
arxivId = {1711.11097},
author = {Zhu, Zhe and Albadawy, Ehab and Saha, Ashirbani and Zhang, Jun and Harowicz, Michael R. and Mazurowski, Maciej A.},
doi = {10.1016/j.compbiomed.2019.04.018},
eprint = {1711.11097},
issn = {18790534},
journal = {Computers in Biology and Medicine},
keywords = {Breast cancer subtype,Deep learning,Radiogenomic},
pmid = {31048129},
title = {{Deep learning for identifying radiogenomic associations in breast cancer}},
year = {2019}
}
@misc{Lecun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
booktitle = {Nature},
doi = {10.1038/nature14539},
issn = {14764687},
pmid = {26017442},
title = {{Deep learning}},
year = {2015}
}
@inproceedings{Girshick2014,
abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30{\%} relative to the previous best result on VOC 2012 - achieving a mAP of 53.3{\%}. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/{\~{}}rbg/rcnn.},
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.81},
eprint = {1311.2524},
isbn = {9781479951178},
issn = {10636919},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
year = {2014}
}
@inproceedings{Girshick2015,
abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
archivePrefix = {arXiv},
arxivId = {1504.08083},
author = {Girshick, Ross},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2015.169},
eprint = {1504.08083},
isbn = {9781467383912},
issn = {15505499},
title = {{Fast R-CNN}},
year = {2015}
}
@article{Ren2017,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features - using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
archivePrefix = {arXiv},
arxivId = {1506.01497},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
doi = {10.1109/TPAMI.2016.2577031},
eprint = {1506.01497},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Object detection,convolutional neural network,region proposal},
pmid = {27295650},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
year = {2017}
}
@inproceedings{Dai2016,
abstract = {We present region-based, fully convolutional networks for accurate and efficient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN [7, 19] that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classifier backbones, such as the latest Residual Networks (ResNets) [10], for object detection. We show competitive results on the PASCAL VOC datasets (e.g., 83.6{\%} mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20× faster than the Faster R-CNN counterpart. Code is made publicly available at: https://github.com/daijifeng001/r-fcn.},
archivePrefix = {arXiv},
arxivId = {1605.06409},
author = {Dai, Jifeng and Li, Yi and He, Kaiming and Sun, Jian},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1605.06409},
issn = {10495258},
title = {{R-FCN: Object detection via region-based fully convolutional networks}},
year = {2016}
}
@article{Hinton2006,
abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind. {\textcopyright} 2006 Massachusetts Institute of Technology.},
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
doi = {10.1162/neco.2006.18.7.1527},
issn = {08997667},
journal = {Neural Computation},
pmid = {16764513},
title = {{A fast learning algorithm for deep belief nets}},
year = {2006}
}
@article{Russakovsky2015,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
eprint = {1409.0575},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
year = {2015}
}
@inproceedings{Gao2019,
abstract = {This study investigates the feasibility of applying state of the art deep learning techniques to detect precancerous stages of squamous cell carcinoma (SCC) cancer in real time to address the challenges while diagnosing SCC with subtle appearance changes as well as video processing speed. Two deep learning models are implemented, which are to determine artefact of video frames and to detect, segment and classify those no-artefact frames respectively. For detection of SCC, both mask-RCNN and YOLOv3 architectures are implemented. In addition, in order to ascertain one bounding box being detected for one region of interest instead of multiple duplicated boxes, a faster non-maxima suppression technique (NMS) is applied on top of predictions. As a result, this developed system can process videos at 16-20 frames per second. Three classes are classified, which are 'suspicious', 'high grade' and 'cancer' of SCC. With the resolution of 1920x1080 pixels of videos, the average processing time while apply YOLOv3 is in the range of 0.064-0.101 seconds per frame, i.e. 10-15 frames per second, while running under Windows 10 operating system with 1 GPU (GeForce GTX 1060). The averaged accuracies for classification and detection are 85{\%} and 74{\%} respectively. Since YOLOv3 only provides bounding boxes, to delineate lesioned regions, mask-RCNN is also evaluated. While better detection result is achieved with 77{\%} accuracy, the classification accuracy is similar to that by YOLOYv3 with 84{\%}. However, the processing speed is more than 10 times slower with an average of 1.2 second per frame due to creation of masks. The accuracy of segmentation by mask-RCNN is 63{\%}. These results are based on the date sets of 350 images. Further improvement is hence in need in the future by collecting, annotating or augmenting more datasets.},
author = {Gao, Xiaohong and Braden, Barbara and Taylor, Stephen and Pang, Wei},
booktitle = {Proceedings - 18th IEEE International Conference on Machine Learning and Applications, ICMLA 2019},
doi = {10.1109/ICMLA.2019.00264},
isbn = {9781728145495},
keywords = {Deep learning,Oesophagus endoscopy,Pre-cancer detection,Real-time video processing,Segmentation},
title = {{Towards real-time detection of squamous pre-cancers from oesophageal endoscopic videos}},
year = {2019}
}
@inproceedings{Choukroun2017,
abstract = {Mammography is the common modality used for screening and early detection of breast cancer. The emergence of machine learning, particularly deep learning methods, aims to assist radiologists to reach higher sensitivity and specificity. Yet, typical supervised machine learning methods demand the radiological images to have findings annotated within the image. This is a tedious task, which is often out of reach due to the high cost and unavailability of expert radiologists. We describe a computer-aided detection and diagnosis system for weakly supervised learning, where the mammogram (MG) images are tagged only on a global level, without local annotations. Our work addresses the problem of MG classification and detection of abnormal findings through a novel deep learning framework built on the multiple instance learning (MIL) paradigm. Our proposed method processes the MG image utilizing the full resolution, with a deep MIL convolutional neural network. This approach allows us to classify the whole MG according to a severity score and localize the source of abnormality in full resolution, while trained on a weakly labeled data set. The key hallmark of our approach is automatic discovery of the discriminating patches in the mammograms using MIL. We validate the proposed method on two mammogram data sets, a large multi-center MG cohort and the publicly available INbreast, in two different scenarios. We present promising results in classification and detection, comparable to a recent supervised method that was trained on fully annotated data set. As the volume and complexity of data in healthcare continues to increase, such an approach may have a profound impact on patient care in many applications.},
author = {Choukroun, Yoni and Bakalo, Ran and Ben-Ari, Rami and Askelrod-Ballin, Ayelet and Barkan, Ella and Kisilev, Pavel},
booktitle = {VCBM 2017 - Eurographics Workshop on Visual Computing for Biology and Medicine},
doi = {10.2312/vcbm.20171232},
isbn = {9783038680369},
title = {{Mammogram classification and abnormality detection from nonlocal labels using deep multiple instance neural network}},
year = {2017}
}
@inproceedings{Zhu2017,
abstract = {Mammogram classification is directly related to computer-aided diagnosis of breast cancer. Traditional methods rely on regions of interest (ROIs) which require great efforts to annotate. Inspired by the success of using deep convolutional features for natural image analysis and multi-instance learning (MIL) for labeling a set of instances/patches, we propose end-to-end trained deep multi-instance networks for mass classification based on whole mammogram without the aforementioned ROIs. We explore three different schemes to construct deep multi-instance networks for whole mammogram classification. Experimental results on the INbreast dataset demonstrate the robustness of proposed networks compared to previous work using segmentation and detection annotations. (Code: https://github.com/wentaozhu/deep-mil-for-whole-mammogram-classification.git ).},
archivePrefix = {arXiv},
arxivId = {1705.08550},
author = {Zhu, Wentao and Lou, Qi and Vang, Yeeleng Scott and Xie, Xiaohui},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-66179-7_69},
eprint = {1705.08550},
isbn = {9783319661780},
issn = {16113349},
keywords = {Deep multi-instance learning,Label assignment-based MIL,Max pooling-based MIL,Sparse MIL,Whole mammogram classification},
title = {{Deep multi-instance networks with sparse label assignment for whole mammogram classification}},
year = {2017}
}
@misc{Oeffinger2015,
abstract = {Importance Breast cancer is a leading cause of premature mortality among US women. Early detection has been shown to be associated with reduced breast cancer morbidity and mortality. Objective To update the American Cancer Society (ACS) 2003 breast cancer screening guideline for women at average risk for breast cancer. Process The ACS commissioned a systematic evidence review of the breast cancer screening literature to inform the update and a supplemental analysis of mammography registry data to address questions related to the screening interval. Formulation of recommendations was based on the quality of the evidence and judgment (incorporating values and preferences) about the balance of benefits and harms. Evidence Synthesis Screening mammography in women aged 40 to 69 years is associated with a reduction in breast cancer deaths across a range of study designs, and inferential evidence supports breast cancer screening for women 70 years and older who are in good health. Estimates of the cumulative lifetime risk of false-positive examination results are greater if screening begins at younger ages because of the greater number of mammograms, as well as the higher recall rate in younger women. The quality of the evidence for overdiagnosis is not sufficient to estimate a lifetime risk with confidence. Analysis examining the screening interval demonstrates more favorable tumor characteristics when premenopausal women are screened annually vs biennially. Evidence does not support routine clinical breast examination as a screeningmethod for women at average risk. Recommendations The ACS recommends that women with an average risk of breast cancer should undergo regular screening mammography starting at age 45 years (strong recommendation).Women aged 45 to 54 years should be screened annually (qualified recommendation).Women 55 years and older should transition to biennial screening or have the opportunity to continue screening annually (qualified recommendation).Women should have the opportunity to begin annual screening between the ages of 40 and 44 years (qualified recommendation).Women should continue screening mammography as long as their overall health is good and they have a life expectancy of 10 years or longer (qualified recommendation). The ACS does not recommend clinical breast examination for breast cancer screening among average-risk women at any age (qualified recommendation). Conclusions and Relevance These updated ACS guidelines provide evidence-based recommendations for breast cancer screening for women at average risk of breast cancer. These recommendations should be considered by physicians and women in discussions about breast cancer screening.},
author = {Oeffinger, Kevin C. and Fontham, Elizabeth T.H. and Etzioni, Ruth and Herzig, Abbe and Michaelson, James S. and Shih, Ya Chen Tina and Walter, Louise C. and Church, Timothy R. and Flowers, Christopher R. and LaMonte, Samuel J. and Wolf, Andrew M.D. and DeSantis, Carol and Lortet-Tieulent, Joannie and Andrews, Kimberly and Manassaram-Baptiste, Deana and Saslow, Debbie and Smith, Robert A. and Brawley, Otis W. and Wender, Richard},
booktitle = {JAMA - Journal of the American Medical Association},
doi = {10.1001/jama.2015.12783},
issn = {15383598},
pmid = {26501536},
title = {{Breast cancer screening for women at average risk: 2015 Guideline update from the American cancer society}},
year = {2015}
}
@inproceedings{M.HeathK.BowyerD.Kopans2001,
abstract = {The Digital Database for Screening Mammography (DDSM) is a resource for use by the mammographic image analysis research community. Primary support for this project was a grant from the Breast Cancer Research Program of the U.S. Army Medical Research and Materiel Command. The DDSM project is a collaborative effort involving co-p.i.s at the Massachusetts General Hospital (D. Kopans, R. Moore), the University of South Florida (K. Bowyer), and Sandia National Laboratories (P. Kegelmeyer). Additional cases from Washington University School of Medicine were provided by Peter E. Shile, MD, Assistant Professor of Radiology and Internal Medicine. Additional collaborating institutions include Wake Forest University School of Medicine (Departments of Medical Engineering and Radiology), Sacred Heart Hospital and ISMD, Incorporated. The primary purpose of the database is to facilitate sound research in the development of computer algorithms to aid in screening. Secondary purposes of the database may include the development of algorithms to aid in the diagnosis and the development of teaching or training aids. The database contains approximately 2,500 studies. Each study includes two images of each breast, along with some associated patient information (age at time of study, ACR breast density rating, subtlety rating for abnormalities, ACR keyword description of abnormalities) and image information (scanner, spatial resolution, ...). Images containing suspicious areas have associated pixel-level "ground truth" information about the locations and types of suspicious regions. Also provided are software both for accessing the mammogram and truth images and for calculating performance figures for automated image analysis algorithms.},
author = {{M. Heath, K. Bowyer, D. Kopans}, R. Moore and P. Kegelmeyer Jr.},
booktitle = {the Fifth International Workshop on Digital Mammography, M.J. Yaffe, ed., Medical Physics Publishing, 2001.},
doi = {ISBN 1-930524-00-5},
title = {{The Digital Database for Screening Mammography}},
year = {2001}
}
@inproceedings{Jamieson2012,
abstract = {Feature extraction is a critical component of medical image analysis. Many computer-aided diagnosis approaches employ hand-designed, heuristic lesion extracted features. An alternative approach is to learn features directly from images. In this preliminary study, we explored the use of Adaptive Deconvolutional Networks (ADN) for learning high-level features in diagnostic breast mass lesion images with potential application to computer-aided diagnosis (CADx) and content-based image retrieval (CBIR). ADNs (Zeiler, et. al., 2011), are recently-proposed unsupervised, generative hierarchical models that decompose images via convolution sparse coding and max pooling. We trained the ADNs to learn multiple layers of representation for two breast image data sets on two different modalities (739 full field digital mammography (FFDM) and 2393 ultrasound images). Feature map calculations were accelerated by use of GPUs. Following Zeiler et. al., we applied the Spatial Pyramid Matching (SPM) kernel (Lazebnik, et. al., 2006) on the inferred feature maps and combined this with a linear support vector machine (SVM) classifier for the task of binary classification between cancer and non-cancer breast mass lesions. Non-linear, local structure preserving dimension reduction, Elastic Embedding (Carreira-Perpi{\~{n}}{\'{a}}n, 2010), was then used to visualize the SPM kernel output in 2D and qualitatively inspect image relationships learned. Performance was found to be competitive with current CADx schemes that use human-designed features, e.g., achieving a 0.632+ bootstrap AUC (by case) of 0.83 [0.78, 0.89] for an ultrasound image set (1125 cases). {\textcopyright} 2012 SPIE.},
author = {Jamieson, Andrew R. and Drukker, Karen and Giger, Maryellen L.},
booktitle = {Medical Imaging 2012: Computer-Aided Diagnosis},
doi = {10.1117/12.910710},
isbn = {9780819489647},
issn = {0277-786X},
title = {{Breast image feature learning with adaptive deconvolutional networks}},
year = {2012}
}
@inproceedings{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
author = {Simonyan, Karen and Zisserman, Andrew},
booktitle = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
title = {{Very deep convolutional networks for large-scale image recognition}},
year = {2015}
}
@inproceedings{Szegedy2016,
abstract = {Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2{\%} top-1 and 5:6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5{\%} top-5 error and 17:3{\%} top-1 error on the validation set and 3:6{\%} top-5 error on the official test set.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.308},
eprint = {1512.00567},
isbn = {9781467388504},
issn = {10636919},
title = {{Rethinking the Inception Architecture for Computer Vision}},
year = {2016}
}
@inproceedings{He2019,
abstract = {Much of the recent progress made in image classification research can be credited to training procedure refinements, such as changes in data augmentations and optimization methods. In the literature, however, most refinements are either briefly mentioned as implementation details or only visible in source code. In this paper, we will examine a collection of such refinements and empirically evaluate their impact on the final model accuracy through ablation study. We will show that, by combining these refinements together, we are able to improve various CNN models significantly. For example, we raise ResNet-50's top-1 validation accuracy from 75.3{\%} to 79.29{\%} on ImageNet. We will also demonstrate that improvement on image classification accuracy leads to better transfer learning performance in other application domains such as object detection and semantic segmentation.},
archivePrefix = {arXiv},
arxivId = {1812.01187},
author = {He, Tong and Zhang, Zhi and Zhang, Hang and Zhang, Zhongyue and Xie, Junyuan and Li, Mu},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2019.00065},
eprint = {1812.01187},
isbn = {9781728132938},
issn = {10636919},
keywords = {Categorization,Deep Learning,Recognition: Detection,Retrieval},
title = {{Bag of tricks for image classification with convolutional neural networks}},
year = {2019}
}
@inproceedings{Zeiler2014,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets. {\textcopyright} 2014 Springer International Publishing.},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Zeiler, Matthew D. and Fergus, Rob},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {1311.2901},
isbn = {9783319105895},
issn = {16113349},
title = {{Visualizing and understanding convolutional networks}},
year = {2014}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets. {\textcopyright} 2014 Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever and Ruslan Salakhutdinov.},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Deep learning,Model combination,Neural networks,Regularization},
title = {{Dropout: A simple way to prevent neural networks from overfitting}},
year = {2014}
}
@inproceedings{Ioffe2015,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82{\%} top-5 test error, exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
booktitle = {32nd International Conference on Machine Learning, ICML 2015},
eprint = {1502.03167},
isbn = {9781510810587},
title = {{Batch normalization: Accelerating deep network training by reducing internal covariate shift}},
year = {2015}
}
@inproceedings{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
booktitle = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
eprint = {1412.6980},
title = {{Adam: A method for stochastic optimization}},
year = {2015}
}
@misc{Pan2010,
abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research. {\textcopyright} 2006 IEEE.},
author = {Pan, Sinno Jialin and Yang, Qiang},
booktitle = {IEEE Transactions on Knowledge and Data Engineering},
doi = {10.1109/TKDE.2009.191},
issn = {10414347},
keywords = {Transfer learning,data mining.,machine learning,survey},
title = {{A survey on transfer learning}},
year = {2010}
}
@inproceedings{Tan2018,
abstract = {As a new classification platform, deep learning has recently received increasing attention from researchers and has been successfully applied to many domains. In some domains, like bioinformatics and robotics, it is very difficult to construct a large-scale well-annotated dataset due to the expense of data acquisition and costly annotation, which limits its development. Transfer learning relaxes the hypothesis that the training data must be independent and identically distributed (i.i.d.) with the test data, which motivates us to use transfer learning to solve the problem of insufficient training data. This survey focuses on reviewing the current researches of transfer learning by using deep neural network and its applications. We defined deep transfer learning, category and review the recent research works based on the techniques used in deep transfer learning.},
author = {Tan, Chuanqi and Sun, Fuchun and Kong, Tao and Zhang, Wenchang and Yang, Chao and Liu, Chunfang},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-01424-7_27},
isbn = {9783030014230},
issn = {16113349},
keywords = {Deep transfer learning,Survey,Transfer learning},
title = {{A survey on deep transfer learning}},
year = {2018}
}
@inproceedings{Ciresan2012,
abstract = {We address a central problem of neuroanatomy, namely, the automatic segmentation of neuronal structures depicted in stacks of electron microscopy (EM) images. This is necessary to efficiently map 3D brain structure and connectivity. To segment biological neuron membranes, we use a special type of deep artificial neural network as a pixel classifier. The label of each pixel (membrane or nonmembrane) is predicted from raw pixel values in a square window centered on it. The input layer maps each window pixel to a neuron. It is followed by a succession of convolutional and max-pooling layers which preserve 2D information and extract features with increasing levels of abstraction. The output layer produces a calibrated probability for each class. The classifier is trained by plain gradient descent on a 512 × 512 × 30 stack with known ground truth, and tested on a stack of the same size (ground truth unknown to the authors) by the organizers of the ISBI 2012 EM Segmentation Challenge. Even without problem-specific postprocessing, our approach outperforms competing techniques by a large margin in all three considered metrics, i.e. rand error, warping error and pixel error. For pixel error, our approach is the only one outperforming a second human observer.},
author = {Cireşan, Dan C. and Giusti, Alessandro and Gambardella, Luca M. and Schmidhuber, J{\"{u}}rgen},
booktitle = {Advances in Neural Information Processing Systems},
isbn = {9781627480031},
issn = {10495258},
title = {{Deep neural networks segment neuronal membranes in electron microscopy images}},
year = {2012}
}
@article{Chen2015,
abstract = {MXNet is a multi-language machine learning (ML) library to ease the develop-ment of ML algorithms, especially for deep neural networks. Embedded in the host language, it blends declarative symbolic expression with imperative tensor computation. It offers auto differentiation to derive gradients. MXNet is compu-tation and memory efficient and runs on various heterogeneous systems, ranging from mobile devices to distributed GPU clusters. This paper describes both the API design and the system implementation of MXNet, and explains how embedding of both symbolic expression and tensor operation is handled in a unified fashion. Our preliminary experiments reveal promising results on large scale deep neural network applications using multiple GPU machines.},
archivePrefix = {arXiv},
arxivId = {arXiv:1512.01274v1},
author = {Chen, Tianqi and Li, Mu and Cmu, U Washington and Li, Yutian and Lin, Min and Wang, Naiyan and Wang, Minjie and Xu, Bing and Zhang, Chiyuan and Zhang, Zheng and Alberta, U},
doi = {10.1145/2532637},
eprint = {arXiv:1512.01274v1},
isbn = {0360-0300},
issn = {03600300},
journal = {Emerald Group Publishing Limited},
pmid = {95420233},
title = {{MXNet : A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems arXiv : 1512 . 01274v1 [ cs . DC ] 3 Dec 2015}},
year = {2015}
}
@article{Hu2020,
abstract = {The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the 'Squeeze-and-Excitation' (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251 percent, surpassing the winning entry of 2016 by a relative improvement of {\$}{\{}\backslashsim {\}}{\$}∼25 percent. Models and code are available at https://github.com/hujie-frank/SENet.},
archivePrefix = {arXiv},
arxivId = {1709.01507},
author = {Hu, Jie and Shen, Li and Albanie, Samuel and Sun, Gang and Wu, Enhua},
doi = {10.1109/TPAMI.2019.2913372},
eprint = {1709.01507},
issn = {19393539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Squeeze-and-excitation,attention,convolutional neural networks,image representations},
pmid = {31034408},
title = {{Squeeze-and-Excitation Networks}},
year = {2020}
}
@inproceedings{Abadi2016,
abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, generalpurpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that Tensor-Flow achieves for several real-world applications.},
archivePrefix = {arXiv},
arxivId = {1605.08695},
author = {Abadi, Mart{\'{i}}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
booktitle = {Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2016},
eprint = {1605.08695},
isbn = {9781931971331},
title = {{TensorFlow: A system for large-scale machine learning}},
year = {2016}
}
@inproceedings{Glorot2011,
abstract = {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros, which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training. Copyright 2011 by the authors.},
author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
booktitle = {Journal of Machine Learning Research},
issn = {15324435},
title = {{Deep sparse rectifier neural networks}},
year = {2011}
}
@inproceedings{Paszke2017,
abstract = {In this article, we describe an automatic differentiation module of PyTorch-a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd [4], and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.},
author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and Facebook, Zachary Devito and Research, A I and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Srl, Orobix and Lerer, Adam},
booktitle = {Advances in Neural Information Processing Systems},
title = {{Automatic differentiation in PyTorch}},
year = {2017}
}
@inproceedings{Liu2020,
abstract = {Bidirectional Encoder Representations from Transformers (BERT; Devlin et al. 2019) represents the latest incarnation of pretrained language models which have recently advanced a wide range of natural language processing tasks. In this paper, we showcase how BERT can be usefully applied in text summarization and propose a general framework for both extractive and abstractive models. We introduce a novel document-level encoder based on BERT which is able to express the semantics of a document and obtain representations for its sentences. Our extractive model is built on top of this encoder by stacking several inter-sentence Transformer layers. For abstractive summarization, we propose a new fine-tuning schedule which adopts different optimizers for the encoder and the decoder as a means of alleviating the mismatch between the two (the former is pretrained while the latter is not). We also demonstrate that a two-staged fine-tuning approach can further boost the quality of the generated summaries. Experiments on three datasets show that our model achieves state-of-the-art results across the board in both extractive and abstractive settings.},
archivePrefix = {arXiv},
arxivId = {1908.08345},
author = {Liu, Yang and Lapata, Mirella},
booktitle = {EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference},
doi = {10.18653/v1/d19-1387},
eprint = {1908.08345},
isbn = {9781950737901},
title = {{Text summarization with pretrained encoders}},
year = {2020}
}
@inproceedings{He2016,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.90},
eprint = {1512.03385},
isbn = {9781467388504},
issn = {10636919},
title = {{Deep residual learning for image recognition}},
year = {2016}
}
@inproceedings{Jia2014,
abstract = {Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying generalpurpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (≈ 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.},
archivePrefix = {arXiv},
arxivId = {1408.5093},
author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
booktitle = {MM 2014 - Proceedings of the 2014 ACM Conference on Multimedia},
doi = {10.1145/2647868.2654889},
eprint = {1408.5093},
isbn = {9781450330633},
keywords = {Computer vision,Machine learning,Neural networks,Open source,Parallel computation},
title = {{Caffe: Convolutional architecture for fast feature embedding}},
year = {2014}
}
@inproceedings{Moreira2018,
abstract = {The increased use of microcomputers and smart-phones has contributed to social progress, but it has also facilitated the exchange of illegal files, such as child pornography photographs, increasing the demand for digital forensic examination in these devices, which can store more than 300,000 images. Based on the large number of files to be analyzed, it is necessary to use capable algorithms to perform this detection, especially in the most challenging scenarios, such as the distinction between pornographic and bikini images. In this work, we present an approach that improves the ''Algorithm for detection of Nudity proposed by Ap-Apid. Our method improved the performance of this algorithm by using machine learning in extracted features from detected skin regions instead of using static rules to classify this kind of images. In addition, we used detected faces as features in order to mitigate false positives in portrait photos. For conducting the training, validation and testing phases, we used the AIIA-PID4 pornographic data set. Furthermore, we also present a statistical analysis by comparing our approach to two algorithms, one based on the 'Algorithm for detection of Nudity' and another proposed by the AIIA-PID4 pornographic data set author. The experimental results showed that we achieved an accuracy of 96.96{\%} and 94.94 in the F1-score metric, increasing the accuracy by 79.19{\%} and 18.21{\%} compared to the referred works, respectively.},
author = {Moreira, Danilo Coura and Fechine, Joseana Ma{\^{c}}Edo},
booktitle = {Proceedings of the International Joint Conference on Neural Networks},
doi = {10.1109/IJCNN.2018.8489100},
isbn = {9781509060146},
keywords = {Computer Forensics,Image Processing,Nudity Detection,Pattern Recognition,Pornography Detection,Skin Detection},
title = {{A Machine Learning-based Forensic Discriminator of Pornographic and Bikini Images}},
year = {2018}
}
@ARTICLE{9112355,
author={Y. {Li} and R. {Zhou} and R. {Xu} and J. {Luo} and S. {Jiang}},
journal={IEEE Transactions on Emerging Topics in Computing}, 
title={A quantum mechanics-based framework for EEG signal feature extraction and classification}, 
year={2020},
volume={},
number={},
pages={1-1}
}
@article{Li_2020,
abstract = {Deep learning achieves unprecedented success involves many fields, whereas the high requirement of memory and time efficiency tolerance have been the intractable challenges for a long time. On the other hand, quantum computing shows its superiorities in some computation problems owing to its intrinsic properties of superposition and entanglement, which may provide a new path to settle these issues. In this paper, a quantum deep convolutional neural network (QDCNN) model based on the quantum parameterized circuit for image recognition is investigated. In analogy to the classical deep convolutional neural network (DCNN), the architecture that a sequence of quantum convolutional layers followed by a quantum classified layer is illustrated. Inspired by the variational quantum algorithms, a quantum–classical hybrid training scheme is demonstrated for the parameter updating in the QDCNN. The network complexity analysis indicates the proposed model provides the exponential acceleration comparing with the classical counterpart. Furthermore, the MNIST and GTSRB datasets are employed to numerical simulation and the quantitative experimental results verify the feasibility and validity.},
author = {Li, YaoChong and Zhou, Ri-Gui and Xu, RuQing and Luo, Jia and Hu, WenWen},
doi = {10.1088/2058-9565/ab9f93},
journal = {Quantum Science and Technology},
number = {4},
pages = {44003},
publisher = {{\{}IOP{\}} Publishing},
title = {{A quantum deep convolutional neural network for image recognition}},
url = {https://doi.org/10.1088{\%}2F2058-9565{\%}2Fab9f93},
volume = {5},
year = {2020}
}